{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheat sheet: https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
    "Paper: https://arxiv.org/pdf/1706.03762.\n",
    "Annotated transformer: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "Matrices $Q, K, V$ have dimensions (batch, seq, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot Product size with input size torch.Size([32, 512, 1024]): torch.Size([32, 512, 1024])\n",
      "Single Head Attention size with input size torch.Size([32, 512, 1024]): torch.Size([32, 512, 8])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Implement Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    QKt = torch.bmm(Q, torch.transpose(K, 1, 2))\n",
    "    d_k = K.size(dim=1)\n",
    "    scaled_qk = torch.sqrt(QKt / d_k)\n",
    "    softmaxed_scaled_qk = torch.softmax(scaled_qk, dim=1)  # https://twitter.com/hardmaru/status/1359323333720875008?s=20&t=P--bktXSUWO0rsPS1cU-7A\n",
    "    return torch.bmm(softmaxed_scaled_qk, V)\n",
    "\n",
    "Q = torch.rand(32, 512, 1024)\n",
    "K = torch.rand(32, 512, 1024)\n",
    "V = torch.rand(32, 512, 1024)\n",
    "output = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Scaled Dot Product size with input size {Q.size()}: {output.size()}\")\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.Wq = nn.Parameter(torch.empty(n_features, hidden_size))\n",
    "        nn.init.trunc_normal_(self.Wq)\n",
    "        self.Wk = nn.Parameter(torch.empty(n_features, hidden_size))\n",
    "        nn.init.trunc_normal_(self.Wk)\n",
    "        self.Wv = nn.Parameter(torch.empty(n_features, hidden_size))\n",
    "        nn.init.trunc_normal_(self.Wv)\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(Q, K, V):\n",
    "        QKt = torch.bmm(Q, torch.transpose(K, 1, 2))\n",
    "        d_k = K.size(dim=1)\n",
    "        scaled_qk = torch.sqrt(QKt / d_k)\n",
    "        softmaxed_scaled_qk = torch.softmax(scaled_qk, dim=1)  # https://twitter.com/hardmaru/status/1359323333720875008?s=20&t=P--bktXSUWO0rsPS1cU-7A\n",
    "        return torch.bmm(softmaxed_scaled_qk, V)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Q = torch.matmul(X, self.Wq)\n",
    "        K = torch.matmul(X, self.Wk)\n",
    "        V = torch.matmul(X, self.Wv)\n",
    "        Z = self.scaled_dot_product_attention(Q, K, V)\n",
    "        return Z\n",
    "\n",
    "X = torch.rand(32, 512, 1024)\n",
    "single_attention_head = AttentionHead(n_features=X.size(dim=2), hidden_size=8)\n",
    "output = single_attention_head(X)\n",
    "print(f\"Single Head Attention size with input size {X.size()}: {output.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-headed Attention size with input size torch.Size([32, 512, 1024]): torch.Size([32, 512, 1024])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_features, hidden_size):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.attn_heads = [AttentionHead(n_features, hidden_size) for _ in range(n_heads)]\n",
    "        self.Wo = nn.Parameter(torch.empty(hidden_size * n_heads, n_features))\n",
    "        nn.init.trunc_normal_(self.Wo)\n",
    "\n",
    "    def forward(self, X):\n",
    "        attn_outputs = torch.cat([head(X) for head in self.attn_heads], dim=2)\n",
    "        Z = torch.matmul(attn_outputs, self.Wo)\n",
    "        return Z\n",
    "\n",
    "X = torch.rand(32, 512, 1024)\n",
    "multi_attention_head = MultiHeadedAttention(n_heads=8, n_features=X.size(dim=2), hidden_size=8)\n",
    "output = multi_attention_head(X)\n",
    "print(f\"Multi-headed Attention size with input size {X.size()}: {output.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_heads, n_features, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attn = MultiHeadedAttention(n_heads, n_features, hidden_size)\n",
    "        self.layer_norm1 = nn.LayerNorm()\n",
    "        self.linear = nn.Linear()\n",
    "        self.layer_norm2 = nn.LayerNorm()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = self.self_attn(X)\n",
    "        Z = self.layer_norm1(torch.add(X, Z))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d774fe2e151afa920deb1d20549b2adac5936a3314ef467b3ac9a255facea646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
